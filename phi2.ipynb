{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### load model ###",
   "id": "f553e0636cfd14d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T08:43:25.586660Z",
     "start_time": "2025-07-25T08:43:25.582394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn as nn"
   ],
   "id": "b9d12ae25bce3e52",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cpu\")\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## telechargement local du model LLM ##",
   "id": "c5a31463fa9265d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T09:37:27.752202Z",
     "start_time": "2025-07-10T09:37:27.328972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sauvegarde locale dans un dossier à toi\n",
    "local_dir = \"./phi2_local\"\n",
    "model.save_pretrained(local_dir)\n",
    "tokenizer.save_pretrained(local_dir)\n"
   ],
   "id": "fd2fa1f12d3622fd",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Sauvegarde locale dans un dossier à toi\u001B[39;00m\n\u001B[32m      2\u001B[39m local_dir = \u001B[33m\"\u001B[39m\u001B[33m./phi2_local\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mmodel\u001B[49m.save_pretrained(local_dir)\n\u001B[32m      4\u001B[39m tokenizer.save_pretrained(local_dir)\n",
      "\u001B[31mNameError\u001B[39m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T08:33:08.894760Z",
     "start_time": "2025-07-25T08:33:06.500806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "local_dir = \"./phi2_local\"  # ou chemin absolu\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)"
   ],
   "id": "f7445ff0e65bca4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eaa361620f8f4d1d9400e3177f918658"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-07-11T08:03:03.747013100Z",
     "start_time": "2025-07-11T07:30:19.585324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Chargement du dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"C:/Users/hugom/OneDrive/Documents/Stage_2025/LLM_dataset.jsonl\"}, split=\"train\")\n",
    "\n",
    "# Prétraitement\n",
    "def format_example(example):\n",
    "    prompt = f\"{example['instruction']}\\n\\nInput:\\n{example['input']}\\n\\nAnswer:\\n\"\n",
    "    full_text = prompt + example[\"output\"]\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "dataset = dataset.map(format_example)\n",
    "local_dir = \"./phi2_local\"\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512,)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True,)\n",
    "\n",
    "# Configuration d'entraînement\n",
    "# Arguments d'entraînement simples\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_llm\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Lancement du fine-tuning\n",
    "trainer.train()\n",
    "trainer.save_model(\"./fine_tuned_phi\")"
   ],
   "id": "e6c1097ebf8d8a1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5629a1daae7a4c939bd2ab62918f1c0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugom\\AppData\\Local\\Temp\\ipykernel_10184\\2034161181.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\hugom\\OneDrive\\Documents\\Stage_2025\\Stage_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5c1ea6787803492d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T08:43:32.526268Z",
     "start_time": "2025-07-25T08:43:32.520119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=283, hidden_dims=[128, 64], num_classes=12):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[1], num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ],
   "id": "2667adf34aebb410",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T08:43:35.179691Z",
     "start_time": "2025-07-25T08:43:35.173292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "def prediction(path_model,x,treshold):\n",
    "    model = MLPClassifier()\n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = x.unsqueeze(0)\n",
    "        y = model(input)\n",
    "        y_probs = torch.sigmoid(y)\n",
    "        y_preds = (y_probs > treshold).int()\n",
    "    return y_probs,y_preds"
   ],
   "id": "a5bd2eec9048afd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T12:44:58.556852Z",
     "start_time": "2025-07-28T12:44:58.392491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import json\n",
    "def predict(X,clf):\n",
    "    input = X\n",
    "    input = input.reshape(1, -1)\n",
    "    y_pred = clf.predict(input)\n",
    "    return y_pred\n",
    "\n",
    "clf = joblib.load(\"C:/Users/hugom/OneDrive/Documents/Stage_2025/dev_Cnn/modele_multioutput.pkl\")\n",
    "#import data\n",
    "test = torch.load('C:/Users/hugom/OneDrive/Documents/Stage_2025/dev_Cnn/dataset/temp.pt',weights_only=False)\n",
    "test_past = torch.load('C:/Users/hugom/OneDrive/Documents/Stage_2025/dev_Cnn/dataset/temp_past.pt',weights_only=False)\n",
    "test_past_past = torch.load('C:/Users/hugom/OneDrive/Documents/Stage_2025/dev_Cnn/dataset/temp_past_past.pt',weights_only=False)\n",
    "#import fichier json\n",
    "with open(\"C:/Users\\hugom\\OneDrive\\Documents\\Stage_2025\\data/trends.json\", \"r\") as f:\n",
    "    data_loaded = json.load(f)\n",
    "trends = {'occupation_list':data_loaded['occupation_list'],\n",
    "          'water consumption':data_loaded['water consumption'],\n",
    "          'electricity consumption':data_loaded['electricity consumption']}\n",
    "dj = data_loaded[\"dj\"]\n",
    "temp = data_loaded[\"temp_ext\"]\n",
    "data = np.array(data_loaded[\"commandes\"])\n",
    "data = list(data.reshape(-1))\n",
    "#prediction creation du segment actuellement hybride pcq on a pas les conso mais il manque que ça\n",
    "X = test['input']\n",
    "X = X.reshape(-1)\n",
    "X = X.numpy()\n",
    "conso = X[240]\n",
    "X = X[261:]\n",
    "data.append(conso)\n",
    "for i in temp:\n",
    "    data.append(i)\n",
    "for i in X:\n",
    "    data.append(i)\n",
    "X = np.array(data)\n",
    "\n",
    "y_pred = predict(X,clf)\n",
    "print(y_pred)"
   ],
   "id": "3ae4d8dcdbba23a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T13:41:13.765226Z",
     "start_time": "2025-07-28T13:41:13.752541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an assistant for hotel energy management.\n",
    "\n",
    "Given:\n",
    "- Overheated zones\n",
    "- Overventilated zones\n",
    "- Detected anomalies\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. List all overheated zones.\n",
    "2. List all overventilated zones.\n",
    "3. List all anomalies or leaks that should be checked.\n",
    "4. Explain why these anomalies could be a problem\n",
    "\n",
    "Respond strictly with plain text (no Markdown):\n",
    "\n",
    "\n",
    "Input:\n",
    "Overheated: room 101, meeting room\n",
    "Overventilated: room 102\n",
    "Anomalies: heater on second floor, light on second floor, ventilation disfunction on third floor\n",
    "\n",
    "Answer:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(prompt)\n"
   ],
   "id": "46653e318682743b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant for hotel energy management.\n",
      "\n",
      "Given:\n",
      "- Overheated zones\n",
      "- Overventilated zones\n",
      "- Detected anomalies\n",
      "\n",
      "Your task:\n",
      "\n",
      "1. List all overheated zones.\n",
      "2. List all overventilated zones.\n",
      "3. List all anomalies or leaks that should be checked.\n",
      "4. Explain why these anomalies could be a problem\n",
      "\n",
      "Respond strictly with plain text (no Markdown):\n",
      "\n",
      "\n",
      "Input:\n",
      "Overheated: room 101, meeting room\n",
      "Overventilated: room 102\n",
      "Anomalies: heater on second floor, light on second floor, ventilation disfunction on third floor\n",
      "\n",
      "Answer:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T13:49:51.882033Z",
     "start_time": "2025-07-28T13:41:15.920735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "outputs = model.generate(**inputs,\n",
    "                         do_sample=False,\n",
    "                         max_new_tokens=400,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         pad_token_id=tokenizer.eos_token_id)\n",
    "\n"
   ],
   "id": "2a36059132c1cede",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T13:50:00.832275Z",
     "start_time": "2025-07-28T13:50:00.701078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(len(prompt))\n",
    "import json\n",
    "with open(\"C:/Users\\hugom\\OneDrive\\Documents\\Stage_2025\\data/assistant.json\", \"w+\") as f:\n",
    "    json.dump(response, f, indent=4)\n",
    "    f.close()\n",
    "with open(\"C:/Users\\hugom\\OneDrive\\Documents\\Stage_2025\\data/assistant.json\", \"r\") as f:\n",
    "    data_loaded = json.load(f)\n",
    "    print(data_loaded[:])"
   ],
   "id": "c82eeaef8a0622b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530\n",
      "\n",
      "You are an assistant for hotel energy management.\n",
      "\n",
      "Given:\n",
      "- Overheated zones\n",
      "- Overventilated zones\n",
      "- Detected anomalies\n",
      "\n",
      "Your task:\n",
      "\n",
      "1. List all overheated zones.\n",
      "2. List all overventilated zones.\n",
      "3. List all anomalies or leaks that should be checked.\n",
      "4. Explain why these anomalies could be a problem\n",
      "\n",
      "Respond strictly with plain text (no Markdown):\n",
      "\n",
      "\n",
      "Input:\n",
      "Overheated: room 101, meeting room\n",
      "Overventilated: room 102\n",
      "Anomalies: heater on second floor, light on second floor, ventilation disfunction on third floor\n",
      "\n",
      "Answer:\n",
      "\n",
      "\n",
      "1. Overheated zones: room 101, meeting room\n",
      "2. Overventilated zones: room 102\n",
      "3. Anomalies: heater on second floor, light on second floor, ventilation disfunction on third floor\n",
      "4. These anomalies could be a problem because they indicate that the HVAC system is not functioning properly. The heater on the second floor could be consuming more energy than necessary, leading to higher energy bills. The light on the second floor could be a sign of a faulty sensor, which could lead to unnecessary energy consumption. The ventilation disfunction on the third floor could lead to poor air quality, which could affect the comfort of the guests and potentially lead to health issues.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7482baadf08b3d63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
